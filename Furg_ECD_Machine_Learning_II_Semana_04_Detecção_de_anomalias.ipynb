{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.2"
    },
    "colab": {
      "name": "Furg - ECD - Machine Learning II - Semana 04 - Detecção de anomalias",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pcpiscator/2T2021/blob/main/Furg_ECD_Machine_Learning_II_Semana_04_Detec%C3%A7%C3%A3o_de_anomalias.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W3lskXfyg76O"
      },
      "source": [
        "# Curso de Especialização em Ciência de Dados - FURG\n",
        "## Machine Learning I - Detecção de anomalias\n",
        "### Prof. Marcelo Malheiros\n",
        "\n",
        "Parte do código adaptada de Aurélien Geron (licença Apache-2.0)\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fK6zMK_ig76S"
      },
      "source": [
        "# Inicialização\n",
        "\n",
        "Aqui importamos as bibliotecas fundamentais de Python para este _notebook_:\n",
        "\n",
        "- NumPy: suporte a vetores, matrizes e operações de Álgebra Linear\n",
        "- Matplotlib: biblioteca de visualização de dados\n",
        "- Scikit-Learn: biblioteca com algoritmos de Machine Learning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fr_1GHdog76U"
      },
      "source": [
        "%matplotlib inline\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import sklearn"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "amhDEGVCg76W"
      },
      "source": [
        "# Dataset problemático\n",
        "\n",
        "Aqui voltamos ao _dataset_ problemático em que tanto o algoritmo K-Means como o DBSCAN falharam anteriormente. Os problemas aqui são dois:\n",
        "\n",
        "1. Os dados se organizam em clusters aproximadamente circulares, o que faz com que o K-Means não funcione bem.\n",
        "\n",
        "2. Os dados apresentam densidade continuamente variável, o que faz com que o DBSCAN não consiga segmentar os diversos _clusters_. Ou todas as instância se juntam em um grande _cluster_, ou muitos pequenos grupos são criados."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zQxOvDN8g76W"
      },
      "source": [
        "from sklearn.datasets import make_blobs\n",
        "\n",
        "X1, y1 = make_blobs(n_samples=1000, centers=((4, -4), (0, 0)), random_state=42)\n",
        "X1 = X1.dot(np.array([[0.374, 0.95], [0.732, 0.598]]))\n",
        "X2, y2 = make_blobs(n_samples=250, centers=1, random_state=42)\n",
        "X2 = X2 + [6, -8]\n",
        "X = np.r_[X1, X2]\n",
        "y = np.r_[y1, y2]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v9DXR0eNg76X"
      },
      "source": [
        "def plot_clusters(X, y=None):\n",
        "    plt.figure(figsize=(8, 4))\n",
        "    plt.scatter(X[:, 0], X[:, 1], c=y, s=1)\n",
        "    plt.xlabel('$x_1$', fontsize=14)\n",
        "    plt.ylabel('$x_2$', fontsize=14, rotation=0)\n",
        "    plt.show()\n",
        "    \n",
        "plot_clusters(X)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aA-pYsBwg76Z"
      },
      "source": [
        "# Misturas Gaussianas\n",
        "\n",
        "Vamos então treinar um modelo de Misturas Gaussianas sobre o _dataset_ anterior.\n",
        "\n",
        "A biblioteca Sciki-Learn provê o algoritmo `GaussianMixture`, que infere os parâmetros de cada um dos $k$ _clusters_ solicitados pelo analista, via hiperparâmetro `n_components=k`.\n",
        "\n",
        "**Atenção:** Como o _default_ para o algoritmo `GaussianMixture` é fazer apenas uma tentativa, há o risco de convergir para uma solução ruim. Por isso definimos também o hiperparâmetro `n_init` para fazer 10 tentativas e manter a melhor delas."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5_d25iPGg76a"
      },
      "source": [
        "from sklearn.mixture import GaussianMixture\n",
        "\n",
        "# criação do modelo\n",
        "gm = GaussianMixture(random_state=42, n_components=3, n_init=10)\n",
        "\n",
        "# treinamento\n",
        "gm.fit(X);"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kuY_UG2dg76c"
      },
      "source": [
        "Os _clusters_ são definidos pelos seguintes parâmetros: seus pesos (disponíveis em `.weights_`), suas médias (uma para cada dimensão, disponíveis em `.means_`) e respectivas matrizes de covariância (disponíveis em `.covariances_`).\n",
        "\n",
        "Se formos examinar os valores, todos os três conjuntos de parâmetros se aproximam bastante dos valores usados para gerar esse _dataset_ sintético. Por exemplo, a quantidade de pontos em cada um dos três clusters era de 500, 500 e 250, ou seja, 40%, 40% e mais 20% das instâncias, respectivamente. Esses valores se refletem os pesos encontrados."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NCSYjovfg76d"
      },
      "source": [
        "print('pesos:', gm.weights_)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8GgYIk4yg76e"
      },
      "source": [
        "print('médias:', gm.means_, sep='\\n')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bTzawWMfg76f"
      },
      "source": [
        "print('matrizes de covariância:', gm.covariances_, sep='\\n')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P4GNibjWg76f"
      },
      "source": [
        "Podemos verificar se o algoritmo de fato convergiu para uma solução examinando o valor booleano `.converged_`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CnQcm8c9g76g"
      },
      "source": [
        "print('convergiu?', gm.converged_)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yhRKdkt4g76g"
      },
      "source": [
        "Também podemos checar quantas iterações levou para chegar na solução com `.n_iter_`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p9eBUJ1Eg76h"
      },
      "source": [
        "print('iterações:', gm.n_iter_)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TWenRrBtg76h"
      },
      "source": [
        "Agora podemos usar o modelo para prever a qual _cluster_ cada instância pertence (usando a ideia de _hard clustering_), usando o método `.predict()`:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c5yTBd_Bg76h"
      },
      "source": [
        "print('clusters:', gm.predict(X))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ozvMwj2Cg76i"
      },
      "source": [
        "Ainda podemos estimar as probabilidades de que cada instância tenha originado de cada um dos três _clusters_, chamando o método `predict_proba()`. Isso equivale a fazer _soft clustering_:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lDck0LmRg76i"
      },
      "source": [
        "print('probabilidades:', gm.predict_proba(X), sep='\\n')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vsuprQBkg76j"
      },
      "source": [
        "Este modelo é **generativo**, então possibilita a **síntese de novas instâncias** pertencentes a este _dataset_ com `.sample()`, incluindo os respectivos rótulos. Este é um processo de **amostragem** da distribuição probabilística aprendida:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XngfImY0g76j"
      },
      "source": [
        "# cria seis novas instâncias\n",
        "X_new, y_new = gm.sample(6)\n",
        "print('novas instâncias:', X_new, sep='\\n')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RP_dCDpgg76k"
      },
      "source": [
        "print('rótulos das novas instâncias:', y_new)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m6I0z29Kg76k"
      },
      "source": [
        "# ilustração das novas instâncias\n",
        "plt.figure(figsize=(8, 4))\n",
        "plt.scatter(X[:, 0], X[:, 1], s=1, c='blue')\n",
        "plt.scatter(X_new[:, 0], X_new[:, 1], s=20, c='red')\n",
        "plt.xlabel('$x_1$', fontsize=14)\n",
        "plt.ylabel('$x_2$', fontsize=14, rotation=0)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GfLL4zF3g76l"
      },
      "source": [
        "Também é possível estimar a densidade do modelo em qualquer local. Isso é obtido usando o método `.score_samples()`. Para qualquer instância dada (original ou sintética), o algoritmo de Mistura Gaussiana estima o _log_ da função de **densidade de probabilidade** (_probability density function_ ou PDF) naquele local. Não confunda com o conceito de probabilidade: para a densidade, valores maiores que um ou negativos são permitidos também.\n",
        "\n",
        "Quanto maior for a pontuação, maior será a densidade."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wCxtgiLsg76l"
      },
      "source": [
        "print('densidades:', gm.score_samples(X_new))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L1Fg4Z_Wg76m"
      },
      "source": [
        "# funções auxiliares para desenho\n",
        "\n",
        "from matplotlib.colors import LogNorm\n",
        "\n",
        "def plot_centroids(centroids, weights=None, circle_color='w', cross_color='k'):\n",
        "    if weights is not None:\n",
        "        centroids = centroids[weights > weights.max() / 10]\n",
        "    plt.scatter(centroids[:, 0], centroids[:, 1],\n",
        "                marker='o', s=35, linewidths=8,\n",
        "                color=circle_color, zorder=10, alpha=0.9)\n",
        "    plt.scatter(centroids[:, 0], centroids[:, 1],\n",
        "                marker='x', s=2, linewidths=12,\n",
        "                color=cross_color, zorder=11, alpha=1)\n",
        "\n",
        "def plot_gaussian_mixture(clusterer, X, resolution=1000, show_ylabels=True):\n",
        "    mins = X.min(axis=0) - 0.1\n",
        "    maxs = X.max(axis=0) + 0.1\n",
        "    xx, yy = np.meshgrid(np.linspace(mins[0], maxs[0], resolution),\n",
        "                         np.linspace(mins[1], maxs[1], resolution))\n",
        "    Z = -clusterer.score_samples(np.c_[xx.ravel(), yy.ravel()])\n",
        "    Z = Z.reshape(xx.shape)\n",
        "    plt.contourf(xx, yy, Z, norm=LogNorm(vmin=1.0, vmax=30.0), levels=np.logspace(0, 2, 12))\n",
        "    plt.contour(xx, yy, Z, norm=LogNorm(vmin=1.0, vmax=30.0), levels=np.logspace(0, 2, 12),\n",
        "                linewidths=1, colors='k')\n",
        "    Z = clusterer.predict(np.c_[xx.ravel(), yy.ravel()])\n",
        "    Z = Z.reshape(xx.shape)\n",
        "    C = plt.contour(xx, yy, Z, linewidths=2, colors='r')\n",
        "    for c in C.collections:\n",
        "        c.set_dashes([(0, (2.0, 2.0))])\n",
        "    plt.plot(X[:, 0], X[:, 1], 'k.', markersize=2)\n",
        "    plot_centroids(clusterer.means_, clusterer.weights_)\n",
        "    plt.xlabel('$x_1$', fontsize=14)\n",
        "    if show_ylabels:\n",
        "        plt.ylabel('$x_2$', fontsize=14, rotation=0)\n",
        "    else:\n",
        "        plt.tick_params(labelleft=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C4Deb2OCg76m"
      },
      "source": [
        "Agora vamos representar graficamente as fronteiras de decisão resultantes (usando linhas tracejadas) e contornos de densidade de todo o espaço:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dassiecFg76n"
      },
      "source": [
        "# gráfico\n",
        "plt.figure(figsize=(8, 4))\n",
        "plot_gaussian_mixture(gm, X)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yS7BTe5Eg76n"
      },
      "source": [
        "Em uma situação geral, pode ser necessário reduzir a dificuldade da tarefa, limitando o número de parâmetros que o algoritmo precisa aprender.\n",
        "\n",
        "Uma maneira de fazer isso é limitar as formas e orientações que os _clusters_ podem ter. Isso é feito impondo restrições às matrizes de covariância, definindo o hiperparâmetro `covariance_type` para um dos seguintes valores:\n",
        "\n",
        "- `'full'` (padrão): sem restrição, quando todos os _clusters_ podem assumir qualquer forma elipsoidal, de qualquer tamanho.\n",
        "\n",
        "- `'tied'`: todos os _clusters_ devem ter a mesma forma, que pode ser qualquer elipsóide (ou seja, todos eles compartilham a mesma matriz de covariância).\n",
        "\n",
        "- `'spherical'`: todos os _clusters_ devem ser esféricos, mas podem ter diâmetros diferentes (ou seja, variâncias diferentes).\n",
        "\n",
        "- `'diag'`: os _clusters_ podem assumir qualquer forma elipsoidal de qualquer tamanho, mas os eixos do elipsóide devem ser paralelos aos eixos principais do espaço (ou seja, as matrizes de covariância devem ser diagonais)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ALWeI_aog76n"
      },
      "source": [
        "# modelos com diferentes restrições\n",
        "gm_full = GaussianMixture(random_state=42, n_components=3, n_init=10, covariance_type='full')\n",
        "gm_tied = GaussianMixture(random_state=42, n_components=3, n_init=10, covariance_type='tied')\n",
        "gm_sphe = GaussianMixture(random_state=42, n_components=3, n_init=10, covariance_type='spherical')\n",
        "gm_diag = GaussianMixture(random_state=42, n_components=3, n_init=10, covariance_type='diag')\n",
        "\n",
        "gm_full.fit(X)\n",
        "gm_tied.fit(X)\n",
        "gm_sphe.fit(X)\n",
        "gm_diag.fit(X);"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PgvZh8hQg76o"
      },
      "source": [
        "# função auxiliar\n",
        "def compare_gaussian_mixtures(gm1, gm2, X):\n",
        "    plt.figure(figsize=(12, 4))\n",
        "    plt.subplot(121)\n",
        "    plot_gaussian_mixture(gm1, X)\n",
        "    plt.title(\"covariance_type='{}'\".format(gm1.covariance_type), fontsize=14)\n",
        "    plt.subplot(122)\n",
        "    plot_gaussian_mixture(gm2, X, show_ylabels=False)\n",
        "    plt.title(\"covariance_type='{}'\".format(gm2.covariance_type), fontsize=14)\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uaAC2Hyig76o"
      },
      "source": [
        "compare_gaussian_mixtures(gm_full, gm_tied, X)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v4ybRKbzg76p"
      },
      "source": [
        "compare_gaussian_mixtures(gm_sphe, gm_diag, X)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SHchsMTSg76p"
      },
      "source": [
        "## Deteção de anomalias\n",
        "\n",
        "Misturas gaussianas podem ser usadas para detecção de anomalias: instâncias localizadas em regiões de baixa densidade podem ser consideradas _outliers_.\n",
        "\n",
        "É preciso definir qual limiar de densidade se deseja. Este limiar pode ser definido como um valor absoluto fixo.\n",
        "\n",
        "Porém, é mais conveniente definir o valor com base em um percentil, como por exemplo o valor de densidade que marca como _outliers_ apenas 4% das instâncias, como mostrado abaixo:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eoESTESSg76q"
      },
      "source": [
        "# obtém a densidade de todas as instâncias\n",
        "densidades = gm.score_samples(X)\n",
        "\n",
        "# identifica o limiar para 4% das amostras em regiões menos densas\n",
        "limiar = np.percentile(densidades, 4.0)\n",
        "\n",
        "# seleciona apenas as instâncias com densidade inferior ao limiar\n",
        "anomalias = X[densidades < limiar]\n",
        "normais = X[densidades >= limiar]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UWxZB32pg76q"
      },
      "source": [
        "# gráfico\n",
        "plt.figure(figsize=(8, 4))\n",
        "plot_gaussian_mixture(gm, X)\n",
        "plt.scatter(anomalias[:, 0], anomalias[:, 1], color='r', marker='x')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Nk-omv8g76q"
      },
      "source": [
        "Note que modelos como este de Mistura Gaussiana tentam ajustar todos os dados, incluindo os _outliers_. Então, se houver muitos deles, isso pode afetar a definição do que é \"normal\" pelo modelo, fazendo que alguns valores discrepantes ainda sejam rotulados como normais.\n",
        "\n",
        "**Dica:** Pode ser útil, então ajustar o modelo a primeira vez, usando este apenas para detectar e remover os _outliers_ mais extremos. Em seguida, um novo ajuste de modelo é feito, para um conjunto de dados mais limpo."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ySOSLMl7g76r"
      },
      "source": [
        "## Seleção de hiperparâmetros\n",
        "\n",
        "Com K-Means, podíamos usar a inércia ou a pontuação da silhueta para selecionar o número apropriado de _clusters_.  já com Misturas Gaussianas essas métricas não podem mais ser usadas, pois os _clusters_ não são esféricos ou têm\n",
        "diferentes tamanhos.\n",
        "\n",
        "Em vez disso, podemos escolher ajustes que minimizam as métricas **Bayesian Information Criterion (BIC)** ou **Akaike Information Criterion (AIC)**.\n",
        "\n",
        "Tanto a BIC quanto a AIC penalizam modelos que têm mais parâmetros para aprender (por exemplo, com mais _clusters_) e dão mais importância a modelos que se ajustam bem aos dados.\n",
        "\n",
        "Normalmente ambas as métricas acabam selecionando o mesmo modelo. Quando elas diferem, o modelo selecionado pelo BIC tende a ter misturas gaussianas mais simples (com menos parâmetros) do que aquele selecionado pelo AIC, mas que tende a não ajustar os dados tão bem também."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4IpwJ1UQg76r"
      },
      "source": [
        "# métrica Bayesian Information Criterion\n",
        "gm.bic(X)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_RviL0cMg76r"
      },
      "source": [
        "# métrica Akaike Information Criterion\n",
        "gm.aic(X)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "betMSDQ-g76r"
      },
      "source": [
        "Como fizemos para o K-Means, podemos treinar uma série de modelos apenas variando o hiperparâmetr $k$, medindo as duas métricas paa cada um deles:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9qFJSTRZg76s"
      },
      "source": [
        "# treina vários modelos, com k variando de 1 até 10\n",
        "gm_k = [GaussianMixture(random_state=42, n_components=k, n_init=10).fit(X) for k in range(1, 11)]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VL3w5Mfbg76s"
      },
      "source": [
        "# calcula as métricas BIC e AIC para cada modelo\n",
        "bics = [model.bic(X) for model in gm_k]\n",
        "aics = [model.aic(X) for model in gm_k]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gr7u9W0kg76s"
      },
      "source": [
        "# exibe o gráfico da variação das métricas em função de k\n",
        "plt.figure(figsize=(8, 4))\n",
        "plt.plot(range(1, 11), bics, 'bo-', label='BIC')\n",
        "plt.plot(range(1, 11), aics, 'go--', label='AIC')\n",
        "plt.xlabel('$k$', fontsize=14)\n",
        "plt.ylabel('métrica', fontsize=14)\n",
        "plt.axis([1, 9.5, np.min(aics) - 50, np.max(aics) + 50])\n",
        "plt.annotate('mínimo', xy=(3, bics[2]), xytext=(0.35, 0.6), textcoords='figure fraction',\n",
        "             fontsize=14, arrowprops=dict(facecolor='black', shrink=0.1))\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2ENTCF6mg76t"
      },
      "source": [
        "O procedimento anterior apenas permite encontrar o valor mais adequado do hiperparâmetro `n_components`, para o caso mais geral de formatos dos clusters. Ou seja, usando o valor padrão `full` do hiperparâmetro `covariance_type`.\n",
        "\n",
        "Uma busca mais ampla poderia tambeḿ ser feita, procurando a melhor combinação tanto de $k$ como do formato dos _clusters_."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uxSvsziGg76t"
      },
      "source": [
        "# procura pelo melhor par de hiperparâmetros 'n_components' e 'covariance_type'\n",
        "\n",
        "min_bic = np.infty\n",
        "for covariance_type in ('full', 'tied', 'spherical', 'diag'):\n",
        "    print(f\"testando tipo de covariância '{covariance_type}' com k igual a\", end=' ')\n",
        "    for k in range(1, 11):\n",
        "        print(k, end=' ')\n",
        "        bic = GaussianMixture(random_state=42, n_init=10, n_components=k, \n",
        "                              covariance_type=covariance_type).fit(X).bic(X)\n",
        "        if bic < min_bic:\n",
        "            min_bic = bic\n",
        "            best_k = k\n",
        "            best_covariance_type = covariance_type\n",
        "    print()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ThaCAgPWg76u"
      },
      "source": [
        "print('melhor n_components:', best_k)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R8bEw-VPg76v"
      },
      "source": [
        "print('melhor covariance_type:', best_covariance_type)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vPp6pqu7g76w"
      },
      "source": [
        "## Aprendizado Bayesiano para Misturas Gaussianas \n",
        "\n",
        "Em vez de procurar manualmente pelo número ideal de _clusters_, é possível usar a classe `BayesianGaussianMixture`, que define pesos nulos ou próximos de zero para _clusters_ desnecessários.\n",
        "\n",
        "Basta definir o número de componentes para um valor que se acredita ser **maior do que o número ideal** de _clusters_, e o algoritmo eliminará os _clusters_ desnecessários automaticamente."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dVkjTnN6g76w"
      },
      "source": [
        "from sklearn.mixture import BayesianGaussianMixture\n",
        "\n",
        "# supomos um valor máximo de 10 para o número de clusters\n",
        "bgm = BayesianGaussianMixture(random_state=42, n_components=10, n_init=20)\n",
        "bgm.fit(X);"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ua7hVEdeg76x"
      },
      "source": [
        "De fato, o algoritmo detectou automaticamente que apenas 3 componentes são necessários:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YZuWCAIUg76x"
      },
      "source": [
        "print('pesos:', np.round(bgm.weights_, 2))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A7oXPwIZg76x"
      },
      "source": [
        "# gráfico\n",
        "plt.figure(figsize=(8, 4))\n",
        "plot_gaussian_mixture(bgm, X)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LZ6ayxWgg76y"
      },
      "source": [
        "# Limitações\n",
        "\n",
        "Modelos de Mistura Gaussiana funcionam bem em aglomerados com formas elipsoidais, mas se for feito um ajuste em um conjunto de dados com formas diferentes, o resultado pode ser diferente do esperado.\n",
        "\n",
        "Por exemplo, vamos ver o que acontece se usarmos o modelo de Mistura Gaussiana com aprendizado Bayesiano para agrupar o _dataset_ sintético `moons`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uFJNcVXgg76y"
      },
      "source": [
        "from sklearn.datasets import make_moons\n",
        "\n",
        "X_moons, y_moons = make_moons(random_state=42, n_samples=1000, noise=0.05)\n",
        "\n",
        "bgm = BayesianGaussianMixture(random_state=42, n_components=10, n_init=10)\n",
        "bgm.fit(X_moons)\n",
        "\n",
        "print('pesos:', np.round(bgm.weights_, 2))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "33jKYKnAg76z"
      },
      "source": [
        "# gráfico do dataset (à esquerda) e do resultado (à direita)\n",
        "plt.figure(figsize=(12, 4))\n",
        "plt.subplot(121)\n",
        "plt.plot(X_moons[:, 0], X_moons[:, 1], 'k.', markersize=2)\n",
        "plt.xlabel('$x_1$', fontsize=14)\n",
        "plt.ylabel('$x_2$', fontsize=14, rotation=0)\n",
        "plt.subplot(122)\n",
        "plot_gaussian_mixture(bgm, X_moons, show_ylabels=False)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eN4bEDs9g76z"
      },
      "source": [
        "Note que ao invés de detectar dois aglomerados em forma de lua, o algoritmo detectou 8 aglomerados elipsoidais.\n",
        "\n",
        "Porém esta aproximação parece ser adequada quando examinamos as linhas de densidade. Dependendo da aplicação, podemos manter este modelo ou tentar outro algoritmo para este _dataset_."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Vh24acQg76z"
      },
      "source": [
        "# Demonstração: clusterização e detecção de anomalias em fotos de rostos\n",
        "\n",
        "Vamos demonstrar agora as tarefas de clusterização e detecção de anomalias usando o conjunto de dados clássico **Olivetti Faces**.\n",
        "\n",
        "Este _dataset_ contém 400 imagens de faces em tons de cinza, cada uma com resolução de 64 × 64 pixels. Cada imagem é achatada em um vetor de uma dimensão e comprimento 4096. Foram fotografadas 40 pessoas diferentes (10 vezes cada), e a tarefa usual é treinar um modelo que possa prever qual pessoa está representada em cada foto."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sGsX4T9jg760"
      },
      "source": [
        "from sklearn.datasets import fetch_olivetti_faces\n",
        "\n",
        "olivetti = fetch_olivetti_faces()\n",
        "\n",
        "X = olivetti.data\n",
        "y = olivetti.target"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nBTHKcgtg760"
      },
      "source": [
        "classes, quantidade = np.unique(olivetti.target, return_counts=True)\n",
        "print('classes:', classes)\n",
        "print('quantidade:', quantidade)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xeCEKtp6g760"
      },
      "source": [
        "## Preparação dos dados com PCA\n",
        "\n",
        "Como cada instância tem 4096 atributos, podemos tentar acelerar o aprendizado posterior aplicando agora uma tarefa de **redução de dimensionalidade** usando o algoritmo `PCA`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W_c5UPD5g761"
      },
      "source": [
        "from sklearn.decomposition import PCA\n",
        "\n",
        "pca = PCA(n_components=0.99)\n",
        "\n",
        "X_pca = pca.fit_transform(X)\n",
        "\n",
        "print('novo número de dimensões:', pca.n_components_)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VA1N1vrgg761"
      },
      "source": [
        "## Agrupamento com Misturas Gaussianas\n",
        "\n",
        "Aqui vamos comparar a diferença entre usar o algoritmo `BayesianGaussianMixture` (verificando quantos _clusters_ são detectados ao se impor um limite superior de 100 _clusters_) e o algoritmo `GaussianMixture`(com um número fixo de 40 _clusters_).\n",
        "\n",
        "Lembrando, é preciso fazer o treino sobre a versão dos dados com dimensão reduzida, disponíveis em `X_pca`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fh6hVX8yg761"
      },
      "source": [
        "from sklearn.mixture import BayesianGaussianMixture\n",
        "\n",
        "# supomos um valor máximo de 100 para o número de clusters\n",
        "bgm = BayesianGaussianMixture(random_state=42, n_components=100, n_init=10)\n",
        "bgm.fit(X_pca);\n",
        "\n",
        "print('pesos:', np.round(bgm.weights_, 2))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ftBOgasHg762"
      },
      "source": [
        "from sklearn.mixture import GaussianMixture\n",
        "\n",
        "# supomos um valor exato de 40 para o número de clusters\n",
        "gm = GaussianMixture(random_state=42, n_components=40, n_init=10)\n",
        "gm.fit(X_pca);\n",
        "\n",
        "print('pesos:', np.round(gm.weights_, 2))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UshUSM-tg762"
      },
      "source": [
        "Agora vamos visualizar os 40 grupos de faces selecionados pelo último modelo, para verificar se a clusterização foi adequada."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "szebfB3pg762"
      },
      "source": [
        "# função auxiliar\n",
        "def plot_faces(faces, labels, n_cols=20):\n",
        "    faces = faces.reshape(-1, 64, 64)\n",
        "    n_rows = (len(faces) - 1) // n_cols + 1\n",
        "    plt.figure(figsize=(n_cols, n_rows * 1.1))\n",
        "    for index, (face, label) in enumerate(zip(faces, labels)):\n",
        "        plt.subplot(n_rows, n_cols, index + 1)\n",
        "        plt.imshow(face, cmap='gray')\n",
        "        plt.axis('off')\n",
        "        plt.title(label)\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "v2mHlq7-g762"
      },
      "source": [
        "# exibe todas as faces de cada cluster (mostrando o rótulo real acima de cada foto)\n",
        "\n",
        "rótulos = gm.predict(X_pca)\n",
        "# rótulos = bgm.predict(X_pca)\n",
        "\n",
        "for cluster_id in np.unique(rótulos):\n",
        "    print('cluster', cluster_id)\n",
        "    in_cluster = rótulos == cluster_id\n",
        "    faces = X[in_cluster]\n",
        "    labels = y[in_cluster]\n",
        "    plot_faces(faces, labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PHTE7V6Wg763"
      },
      "source": [
        "O resultado de fato agrupa faces bastante similares, mas nem sempre da mesma pessoa. Dependendo da aplicação, poderíamos precisar de mais clusters ou ainda usar _features_ adicionais para aumentar as chances de identificar a mesma pessoa em poses diferentes."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-ji6vm7Fg763"
      },
      "source": [
        "## Geração de novas instâncias\n",
        "\n",
        "Usando o modelo `gm` treinado anteriormente, podemos gerar algumas novas faces com o método `.sample()`.\n",
        "\n",
        "Note que novas instâncias têm o número reduzido de dimensões, então para visualizá-las precisaremos usar a transformação inversa do PCA, via método `.inverse_transform()`, para transformar essas instâncias sintéricas em imagens."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SSmUfT_jg763"
      },
      "source": [
        "n_gen_faces = 20\n",
        "gen_faces_reduced, y_gen_faces = gm.sample(n_samples=n_gen_faces)\n",
        "gen_faces = pca.inverse_transform(gen_faces_reduced)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "woZDVUxpg764"
      },
      "source": [
        "plot_faces(gen_faces, y_gen_faces, n_cols=10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JwKejWr4g764"
      },
      "source": [
        "## Detecção de faces anômalas\n",
        "\n",
        "Agora iremos criar novas imagens, fazendo um giro, invertendo ou escurecendo imagens originais.\n",
        "\n",
        "Então podemos verificar se o modeloé capaz de detectar estas anomalias. Faremos isso comparando a saída do método `.score_samples()`, que retorna as densidades calculadas, para imagens normais e para as anomalias."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mehJTbK6g764"
      },
      "source": [
        "# cria imagens rotacionadas\n",
        "n_rotated = 4\n",
        "rotated = np.transpose(X[:n_rotated].reshape(-1, 64, 64), axes=[0, 2, 1])\n",
        "rotated = rotated.reshape(-1, 64*64)\n",
        "y_rotated = y[:n_rotated]\n",
        "\n",
        "# cria imagens invertidas verticalmente\n",
        "n_flipped = 3\n",
        "flipped = X[:n_flipped].reshape(-1, 64, 64)[:, ::-1]\n",
        "flipped = flipped.reshape(-1, 64*64)\n",
        "y_flipped = y[:n_flipped]\n",
        "\n",
        "# cria imagens escurecidas\n",
        "n_darkened = 3\n",
        "darkened = X[:n_darkened].copy()\n",
        "darkened[:, 1:-1] *= 0.3\n",
        "y_darkened = y[:n_darkened]\n",
        "\n",
        "X_bad_faces = np.r_[rotated, flipped, darkened]\n",
        "y_bad = np.concatenate([y_rotated, y_flipped, y_darkened])\n",
        "\n",
        "plot_faces(X_bad_faces, y_bad)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5NXKcei8g765"
      },
      "source": [
        "# precisamo reduzir a dimensão destas novas imagens\n",
        "X_bad_faces_pca = pca.transform(X_bad_faces)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zm6gOxuRg765"
      },
      "source": [
        "# agora veremos a densidade calculada para cada uma delas\n",
        "gm.score_samples(X_bad_faces_pca)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BntPbLvtg765"
      },
      "source": [
        "As faces ruins são todas consideradas **altamente improváveis** pelo modelo de Mistura Gaussiana. Vamos comparar esses valores com as pontuações das instâncias originais:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TEazInThg766"
      },
      "source": [
        "gm.score_samples(X_pca[:10])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uEY65Uqjg766"
      },
      "source": [
        "## Detecção de anomalias usando redução de dimensionalidade\n",
        "\n",
        "Para finalizar, vamos agora mostrar que algumas técnicas de redução de dimensionalidade também podem ser usadas para detecção de anomalias.\n",
        "\n",
        "Aqui vamos usar o conjunto de dados Olivetti Faces, já reduzido por PCA e preservando 99% da variação.\n",
        "\n",
        "A estratégia é a seguinte: calcular o **erro de reconstrução para cada imagem**. Vamos examinar as imagens modificadas construídas anteriormente e observar seu erro de reconstrução, que será maior do que para imagens normais."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zFlMWJlbg766"
      },
      "source": [
        "# erro de reconstrução usando Mean Squared Error (MSE)\n",
        "def erro_de_reconstrução(pca, X_original):\n",
        "    X_pca = pca.transform(X_original)\n",
        "    X_reconstruido = pca.inverse_transform(X_pca)\n",
        "    mse = np.square(X_reconstruido - X_original).mean(axis=-1)\n",
        "    return mse"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o7narKbYg767"
      },
      "source": [
        "# erro médio de reconstrução para todas as imagens ruins\n",
        "erro_de_reconstrução(pca, X_bad_faces).mean()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J0maxeaHg768"
      },
      "source": [
        "# erro médio de reconstrução para todas as imagens originais\n",
        "erro_de_reconstrução(pca, X).mean()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R0KyExSrg768"
      },
      "source": [
        "Ao plotarmos uma imagem reconstruída com muito erro, veremos que de fato ela visualmente se afasta bastante de um rosto normal."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bZ0g2HFYg768"
      },
      "source": [
        "# faces ruins feitas anteriormente\n",
        "plot_faces(X_bad_faces, y_bad)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4J6CDurBg769"
      },
      "source": [
        "# faces ruins reconstruidas\n",
        "X_bad_faces_reconstruidas = pca.inverse_transform(X_bad_faces_pca)\n",
        "plot_faces(X_bad_faces_reconstruidas, y_bad)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}